# Multivariate time series prediction using transformers architecture

The following Notebook shows the coding part of my Bachelor Thesis for the Information and Communication Systems and Services 
bachelor degree in the University of Applied Science Technikum Wien.

Author: Sergio Tallo Torres 
Date: April 2022

Abstract:

In December 2017 the advent of the transformer architecture became the most promising solution for Natural Language Processing (NLP) tasks 
and has since become state of the art for these tasks. However, in NLP tasks, words are encoded to vectors in a multidimensional space, 
and if sentences are a sequence of words, every task dealing with sequences of multidimensional vectors should be, in theory, suitable to 
have a solution with a transformer model.
In this work the author will explain the problem that inspire the transformerâ€™s research and the details of its architecture in an easy way, 
will perform a data analysis in a certain data set, and will show a model that, using transformers over a sequence of multidimensional data, 
can forecast which should be the next element of that sequence. With this forecast any user could, in theory, compares it with the real 
measurement and obtain information about possible anomalies before they happen.

This Repository contains the Thesis coding and the actual Thesis as pdf.

Disclaimer:

Any person is free to use any part of this project, no need to ask. I am only asking for a mention of this use and to keep your project public.
Colaboration is what push humanity forward!
